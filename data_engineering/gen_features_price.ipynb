{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2696a408-e122-4cb8-975d-2ea68c2b8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pyspark.sql import functions as f\n",
    "import re\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Dict\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, FloatType\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pyspark.sql.window import Window\n",
    "import plotly.express as px\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from itertools import chain\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from geopy import distance\n",
    "from dateutil.relativedelta import relativedelta\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8e6f4-87db-4e71-9319-99a4ebde0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn --force\n",
    "#!pip install catboost\n",
    "#!pip install ipywidgets --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b137e1b4-3810-4ac3-b641-597e3fc4c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('prepared_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e5a0f3-79ea-47b5-988c-392d6cce6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = df[df['report_date'] > pd.to_datetime('2019-01-01')].groupby('objectid').apply(lambda df: df.merge(pd.DataFrame(\n",
    "                {'report_date': pd.date_range(df['report_date'].min() + MonthEnd(-3), df['report_date'].max(), freq = 'M')})).fillna(\n",
    "                            {col: df[col].mode()[0] if len(df[col].mode()) > 0 else np.nan for col in ['latitude', 'longitude']})).reset_index(drop = True)\n",
    "\n",
    "\n",
    "obj_df = obj_df.assign(year = obj_df['report_date'].map(lambda dt: dt.year), \n",
    "                       quartal = obj_df['report_date'].map(lambda dt: dt.quarter))[['year', 'quartal', 'objectid', 'latitude', 'longitude']]\n",
    "\n",
    "obj_df = obj_df.drop_duplicates(subset = ['year', 'quartal', 'objectid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5187fb72-e9ab-42a7-b9e4-9e1f8bcd38d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objectid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [latitude, longitude]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_df.groupby('objectid').agg({'latitude': 'count', 'longitude': 'count'}).query('latitude != longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba908ab7-540c-49d9-a212-3d7c51c92e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude_part</th>\n",
       "      <th>longtitude_part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.54694</td>\n",
       "      <td>37.590116</td>\n",
       "      <td>55.750337</td>\n",
       "      <td>37.70674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude  latitude_part  longtitude_part\n",
       "0  55.54694  37.590116      55.750337         37.70674"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.concat([obj_df.sample(1)[['latitude','longitude']].reset_index(drop = True), \n",
    "           obj_df.sample(1)[['latitude','longitude']].rename(columns = dict(zip(['latitude','longitude'], ['latitude_part','longtitude_part']))).reset_index(drop = True)], axis = 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66648030-a2ef-42dd-b63f-2079cbda562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(latitude=55.54694, longitude=37.590116, latitude_part=55.750337, longtitude_part=37.70674, dist=23805.865234375, dist1=23777.68290707195)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_area(df):\n",
    "    r_Earth = 6372795\n",
    "    \n",
    "    get_dist = f.udf(lambda c1, c2: distance.geodesic(c1, c2).m, FloatType())\n",
    "    df = df.withColumn('dist', \n",
    "                get_dist(f.array('latitude', 'longitude'), f.array('latitude_part', 'longtitude_part'))\n",
    "                      )\n",
    "    \n",
    "    df = df.withColumn('dist1', \n",
    "                r_Earth*2*f.asin( \n",
    "                    (f.sin( (f.radians('latitude_part') - f.radians('latitude')) / 2 )**2 + \\\n",
    "                     f.cos(f.radians('latitude_part'))*f.cos(f.radians('latitude')) * f.sin( (f.radians('longtitude_part') - f.radians('longitude')) / 2 )**2 )**(1/2)\n",
    "                                )\n",
    "                      )\n",
    "        \n",
    "    return df\n",
    "\n",
    "merge_area(spark.createDataFrame(t)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbef5fb7-025b-4b94-a16b-7794d858376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.table('prod_dm_cian.cd_soba_primary').limit(50).toPandas()#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b1cd0-556a-4351-b45b-8e8ba079259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237851046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cloudera/parcels/AnacondaPy37/envs/python3.7.9/lib/python3.7/site-packages/pyarrow/util.py:43: FutureWarning: pyarrow.open_stream is deprecated as of 0.17.0, please use pyarrow.ipc.open_stream instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "[Stage 5:=========================================>            (154 + 28) / 200]24/04/16 18:07:27 ERROR cluster.YarnScheduler: Lost executor 17 on p0drp0-db5019xp.region.vtb.ru: Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "24/04/16 18:07:28 ERROR cluster.YarnScheduler: Lost executor 19 on p0drp0-db5019xp.region.vtb.ru: Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "24/04/16 18:07:28 ERROR cluster.YarnScheduler: Lost executor 14 on p0drp0-db5019xp.region.vtb.ru: Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "24/04/16 18:07:28 ERROR cluster.YarnScheduler: Lost executor 16 on p0drp0-db5019xp.region.vtb.ru: Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "[Stage 5:=========================================>            (155 + 28) / 200]24/04/16 18:07:30 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 17 for reason Container marked as failed: container_e336_1710544669313_22661_01_000021 on host: p0drp0-db5019xp.region.vtb.ru. Exit status: 1. Diagnostics: [2024-04-16 18:07:28.243]Exception from container-launch.\n",
      "Container id: container_e336_1710544669313_22661_01_000021\n",
      "Exit code: 1\n",
      "Shell output: main : command provided 1\n",
      "main : run as user is vtb70175290\n",
      "main : requested yarn user is vtb70175290\n",
      "Getting exit code file...\n",
      "Creating script paths...\n",
      "Writing pid file...\n",
      "Writing to tmp file /srv/hadoop-hdfs/data01/yarn/nm/nmPrivate/application_1710544669313_22661/container_e336_1710544669313_22661_01_000021/container_e336_1710544669313_22661_01_000021.pid.tmp\n",
      "Writing to cgroup task files...\n",
      "Creating local dirs...\n",
      "Launching container...\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:28.264]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-bcb52d16-465f-46b9-9283-0a7f2ad0dcfe\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data09/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-9aba79d0-15e3-41a1-96f7-a345248dc87c\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data07/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-6a8bfd18-3e02-45f8-a1e9-f53a03acdc89\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data04/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-9764f1ee-b520-4d75-8fb3-68d05fc322d6\n",
      "24/04/16 18:07:27 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:27 INFO executor.Executor: Starting executor ID 17 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:27 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:27 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:28.264]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-bcb52d16-465f-46b9-9283-0a7f2ad0dcfe\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data09/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-9aba79d0-15e3-41a1-96f7-a345248dc87c\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data07/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-6a8bfd18-3e02-45f8-a1e9-f53a03acdc89\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data04/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-9764f1ee-b520-4d75-8fb3-68d05fc322d6\n",
      "24/04/16 18:07:27 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:27 INFO executor.Executor: Starting executor ID 17 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:27 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:27 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "24/04/16 18:07:30 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 19 for reason Container marked as failed: container_e336_1710544669313_22661_01_000023 on host: p0drp0-db5019xp.region.vtb.ru. Exit status: 1. Diagnostics: [2024-04-16 18:07:28.396]Exception from container-launch.\n",
      "Container id: container_e336_1710544669313_22661_01_000023\n",
      "Exit code: 1\n",
      "Shell output: main : command provided 1\n",
      "main : run as user is vtb70175290\n",
      "main : requested yarn user is vtb70175290\n",
      "Getting exit code file...\n",
      "Creating script paths...\n",
      "Writing pid file...\n",
      "Writing to tmp file /srv/hadoop-hdfs/data06/yarn/nm/nmPrivate/application_1710544669313_22661/container_e336_1710544669313_22661_01_000023/container_e336_1710544669313_22661_01_000023.pid.tmp\n",
      "Writing to cgroup task files...\n",
      "Creating local dirs...\n",
      "Launching container...\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:28.417]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-7fbe3b9f-b261-4a63-9678-e2e15094bc6d\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data05/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-3bec9ac5-a60a-4756-a0b3-834a96059d8f\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data12/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-4fbab4b7-66a0-46e7-acd9-000ad5c485e0\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data11/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-88bc9c97-0ddd-47e7-9fdf-c175fb3a4838\n",
      "24/04/16 18:07:27 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:27 INFO executor.Executor: Starting executor ID 19 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:27 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:28 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:28.418]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-7fbe3b9f-b261-4a63-9678-e2e15094bc6d\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data05/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-3bec9ac5-a60a-4756-a0b3-834a96059d8f\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data12/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-4fbab4b7-66a0-46e7-acd9-000ad5c485e0\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data11/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-88bc9c97-0ddd-47e7-9fdf-c175fb3a4838\n",
      "24/04/16 18:07:27 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:27 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:27 INFO executor.Executor: Starting executor ID 19 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:27 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:27 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:28 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "24/04/16 18:07:30 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 14 for reason Container marked as failed: container_e336_1710544669313_22661_01_000018 on host: p0drp0-db5019xp.region.vtb.ru. Exit status: 1. Diagnostics: [2024-04-16 18:07:28.822]Exception from container-launch.\n",
      "Container id: container_e336_1710544669313_22661_01_000018\n",
      "Exit code: 1\n",
      "Shell output: main : command provided 1\n",
      "main : run as user is vtb70175290\n",
      "main : requested yarn user is vtb70175290\n",
      "Getting exit code file...\n",
      "Creating script paths...\n",
      "Writing pid file...\n",
      "Writing to tmp file /srv/hadoop-hdfs/data04/yarn/nm/nmPrivate/application_1710544669313_22661/container_e336_1710544669313_22661_01_000018/container_e336_1710544669313_22661_01_000018.pid.tmp\n",
      "Writing to cgroup task files...\n",
      "Creating local dirs...\n",
      "Launching container...\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:28.844]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-d640c118-418e-4f2c-93b2-76da65e71f70\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data10/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-71adff74-f751-4232-951d-ab3c869f9591\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data11/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-383f1c55-c650-428a-bfcf-5c7fba7d8b67\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data05/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-744ac4a6-1712-40b7-9a7b-a04cba204a10\n",
      "24/04/16 18:07:28 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:28 INFO executor.Executor: Starting executor ID 14 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:28 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:28 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:28.844]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-d640c118-418e-4f2c-93b2-76da65e71f70\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data10/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-71adff74-f751-4232-951d-ab3c869f9591\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data11/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-383f1c55-c650-428a-bfcf-5c7fba7d8b67\n",
      "24/04/16 18:07:27 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data05/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-744ac4a6-1712-40b7-9a7b-a04cba204a10\n",
      "24/04/16 18:07:28 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:28 INFO executor.Executor: Starting executor ID 14 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:28 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:28 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "24/04/16 18:07:30 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 16 for reason Container marked as failed: container_e336_1710544669313_22661_01_000020 on host: p0drp0-db5019xp.region.vtb.ru. Exit status: 1. Diagnostics: [2024-04-16 18:07:29.018]Exception from container-launch.\n",
      "Container id: container_e336_1710544669313_22661_01_000020\n",
      "Exit code: 1\n",
      "Shell output: main : command provided 1\n",
      "main : run as user is vtb70175290\n",
      "main : requested yarn user is vtb70175290\n",
      "Getting exit code file...\n",
      "Creating script paths...\n",
      "Writing pid file...\n",
      "Writing to tmp file /srv/hadoop-hdfs/data08/yarn/nm/nmPrivate/application_1710544669313_22661/container_e336_1710544669313_22661_01_000020/container_e336_1710544669313_22661_01_000020.pid.tmp\n",
      "Writing to cgroup task files...\n",
      "Creating local dirs...\n",
      "Launching container...\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:29.040]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-ab88097d-0c8a-4001-9049-a496079b6588\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data05/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-db69f159-05df-4a13-9540-f225e648ebc6\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data09/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-13cdd0f1-a9ec-4373-b418-51dbb61db3f7\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data02/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-1877129f-7bbf-4b5d-afce-3a2272e71abc\n",
      "24/04/16 18:07:28 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:28 INFO executor.Executor: Starting executor ID 16 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:28 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:28 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "[2024-04-16 18:07:29.040]Container exited with a non-zero exit code 1. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "Last 4096 bytes of stderr :\n",
      "290/appcache/application_1710544669313_22661/blockmgr-ab88097d-0c8a-4001-9049-a496079b6588\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data05/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-db69f159-05df-4a13-9540-f225e648ebc6\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data09/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-13cdd0f1-a9ec-4373-b418-51dbb61db3f7\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Created local directory at /srv/hadoop-hdfs/data02/yarn/nm/usercache/vtb70175290/appcache/application_1710544669313_22661/blockmgr-1877129f-7bbf-4b5d-afce-3a2272e71abc\n",
      "24/04/16 18:07:28 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.182.77.45:30267\n",
      "24/04/16 18:07:28 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "24/04/16 18:07:28 INFO executor.Executor: Starting executor ID 16 on host p0drp0-db5019xp.region.vtb.ru\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61774. Attempting port 61775.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61775. Attempting port 61776.\n",
      "24/04/16 18:07:28 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 61776. Attempting port 61777.\n",
      "24/04/16 18:07:28 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "java.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 3 retries (starting from 61774)! Consider explicitly setting the appropriate port for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:444)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:436)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/04/16 18:07:28 INFO storage.DiskBlockManager: Shutdown hook called\n",
      "24/04/16 18:07:28 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "[Stage 5:==================================================>   (188 + 12) / 200]"
     ]
    }
   ],
   "source": [
    "def merge_area_price(esc, df):\n",
    "    df = esc.join(df, on = ['year', 'quartal'], how = 'left')\n",
    "    print(df.count())\n",
    "    r_Earth = 6372795\n",
    "    \n",
    "    get_dist = f.udf(lambda c1, c2: distance.geodesic(c1, c2).m, FloatType())\n",
    "    df = df.withColumn('dist', \n",
    "                get_dist(f.array('latitude', 'longitude'), f.array('latitude_part', 'longtitude_part'))\n",
    "                      )\n",
    "    #df = df.withColumn('dist', \n",
    "    #            r_Earth*2*f.asin(f.sin( (f.radians('latitude_part') - f.radians('latitude')) / 2 )**2 + \\\n",
    "    #                     f.cos(f.radians('latitude_part'))*f.cos(f.radians('latitude')) * f.sin( (f.radians('longtitude_part') - f.radians('longitude')) / 2 )**2 )\n",
    "    #                  )\n",
    "    w  = Window.partitionBy('objectid', 'year', 'quartal').orderBy('dist')\n",
    "    df = df.withColumn('dist_rank', f.dense_rank().over(w))\n",
    "    df = df.filter(f.col('dist_rank') <= 50)\n",
    "    return df\n",
    "dfp = merge_area_price(spark.createDataFrame(obj_df), spark.table('prod_dm_cian.cd_soba_primary')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68ca41b6-6242-405e-bbf8-a25add18cbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                        0\n",
       "quartal                     0\n",
       "objectid                    0\n",
       "latitude                    0\n",
       "longitude                   0\n",
       "jk_id                       0\n",
       "jk_nm                       0\n",
       "latitude_part               0\n",
       "longtitude_part             0\n",
       "city                        0\n",
       "street                      0\n",
       "house                       0\n",
       "district                    0\n",
       "class_nm                    0\n",
       "rooms_cnt                   0\n",
       "area_range_part             0\n",
       "flat_cnt                    0\n",
       "total_area_part             0\n",
       "offer_price_sqm_amt         0\n",
       "offer_currency_price_nm     0\n",
       "developer_nm                0\n",
       "currency_price_nm           0\n",
       "deadline_actual             0\n",
       "deadline_change             0\n",
       "deadline_plan               0\n",
       "count_fl_cnt                0\n",
       "count_ul_cnt                0\n",
       "bought_price_sqm_info       0\n",
       "currency_bought_price_nm    0\n",
       "source_nm                   0\n",
       "system_dt                   0\n",
       "dist                        0\n",
       "dist_rank                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c012d87c-5bc5-4d0e-a3b2-9586e4277a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp['dist_rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef832d6b-8197-4fdb-a2a2-a1bfe318aace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quartal</th>\n",
       "      <th>objectid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>jk_id</th>\n",
       "      <th>jk_nm</th>\n",
       "      <th>latitude_part</th>\n",
       "      <th>longtitude_part</th>\n",
       "      <th>city</th>\n",
       "      <th>street</th>\n",
       "      <th>house</th>\n",
       "      <th>district</th>\n",
       "      <th>class_nm</th>\n",
       "      <th>rooms_cnt</th>\n",
       "      <th>area_range_part</th>\n",
       "      <th>flat_cnt</th>\n",
       "      <th>total_area_part</th>\n",
       "      <th>offer_price_sqm_amt</th>\n",
       "      <th>offer_currency_price_nm</th>\n",
       "      <th>developer_nm</th>\n",
       "      <th>currency_price_nm</th>\n",
       "      <th>deadline_actual</th>\n",
       "      <th>deadline_change</th>\n",
       "      <th>deadline_plan</th>\n",
       "      <th>count_fl_cnt</th>\n",
       "      <th>count_ul_cnt</th>\n",
       "      <th>bought_price_sqm_info</th>\n",
       "      <th>currency_bought_price_nm</th>\n",
       "      <th>source_nm</th>\n",
       "      <th>system_dt</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2036</td>\n",
       "      <td>2</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>54.663040</td>\n",
       "      <td>39.656265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2029</td>\n",
       "      <td>2</td>\n",
       "      <td>22768.0</td>\n",
       "      <td>54.614542</td>\n",
       "      <td>39.697427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2029</td>\n",
       "      <td>2</td>\n",
       "      <td>27498.0</td>\n",
       "      <td>45.063797</td>\n",
       "      <td>39.143150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2029</td>\n",
       "      <td>2</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>54.663040</td>\n",
       "      <td>39.656265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>56.919074</td>\n",
       "      <td>53.261569</td>\n",
       "      <td>1520737.0</td>\n",
       "      <td>None</td>\n",
       "      <td>56.91979</td>\n",
       "      <td>53.26272</td>\n",
       "      <td></td>\n",
       "      <td> </td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td> 30  50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37778.000000</td>\n",
       "      <td>rur</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>56.919074</td>\n",
       "      <td>53.261569</td>\n",
       "      <td>51178.0</td>\n",
       "      <td> </td>\n",
       "      <td>56.87181</td>\n",
       "      <td>53.24156</td>\n",
       "      <td></td>\n",
       "      <td> 10  </td>\n",
       "      <td>  8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td> 50  70</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.866667</td>\n",
       "      <td>53670.666667</td>\n",
       "      <td>rur</td>\n",
       "      <td></td>\n",
       "      <td>  </td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2.284201</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>56.919074</td>\n",
       "      <td>53.261569</td>\n",
       "      <td>48534.0</td>\n",
       "      <td></td>\n",
       "      <td>56.87181</td>\n",
       "      <td>53.24156</td>\n",
       "      <td></td>\n",
       "      <td> 10  </td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td> 30  50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>50431.000000</td>\n",
       "      <td>rur</td>\n",
       "      <td> </td>\n",
       "      <td>  </td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-04-01, 2019-10-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2.284201</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>56.919074</td>\n",
       "      <td>53.261569</td>\n",
       "      <td>48534.0</td>\n",
       "      <td></td>\n",
       "      <td>56.87181</td>\n",
       "      <td>53.24156</td>\n",
       "      <td></td>\n",
       "      <td> 10  </td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td> 50  70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>50111.500000</td>\n",
       "      <td>rur</td>\n",
       "      <td> </td>\n",
       "      <td>  </td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-04-01, 2019-10-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2.284201</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>56.919074</td>\n",
       "      <td>53.261569</td>\n",
       "      <td>38335.0</td>\n",
       "      <td></td>\n",
       "      <td>56.87181</td>\n",
       "      <td>53.24156</td>\n",
       "      <td></td>\n",
       "      <td> 10  </td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td> 30  50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.733333</td>\n",
       "      <td>49899.333333</td>\n",
       "      <td>rur</td>\n",
       "      <td></td>\n",
       "      <td> </td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2.284201</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>56.919074</td>\n",
       "      <td>53.261569</td>\n",
       "      <td>42077.0</td>\n",
       "      <td></td>\n",
       "      <td>56.87181</td>\n",
       "      <td>53.24156</td>\n",
       "      <td></td>\n",
       "      <td> 10  </td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td> 50  70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>26564.500000</td>\n",
       "      <td>rur</td>\n",
       "      <td></td>\n",
       "      <td> </td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2.284201</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  quartal  objectid   latitude  longitude      jk_id           jk_nm  \\\n",
       "0   2036        2   30390.0  54.663040  39.656265        NaN            None   \n",
       "1   2029        2   22768.0  54.614542  39.697427        NaN            None   \n",
       "2   2029        2   27498.0  45.063797  39.143150        NaN            None   \n",
       "3   2029        2   30390.0  54.663040  39.656265        NaN            None   \n",
       "4   2019        1    1802.0  56.919074  53.261569  1520737.0            None   \n",
       "..   ...      ...       ...        ...        ...        ...             ...   \n",
       "95  2019        1    1802.0  56.919074  53.261569    51178.0      \n",
       "96  2019        1    1802.0  56.919074  53.261569    48534.0              \n",
       "97  2019        1    1802.0  56.919074  53.261569    48534.0              \n",
       "98  2019        1    1802.0  56.919074  53.261569    38335.0         \n",
       "99  2019        1    1802.0  56.919074  53.261569    42077.0          \n",
       "\n",
       "    latitude_part  longtitude_part    city                street    house  \\\n",
       "0             NaN              NaN    None                  None     None   \n",
       "1             NaN              NaN    None                  None     None   \n",
       "2             NaN              NaN    None                  None     None   \n",
       "3             NaN              NaN    None                  None     None   \n",
       "4        56.91979         53.26272            10   \n",
       "..            ...              ...     ...                   ...      ...   \n",
       "95       56.87181         53.24156     10      8   \n",
       "96       56.87181         53.24156     10         \n",
       "97       56.87181         53.24156     10         \n",
       "98       56.87181         53.24156     10          1   \n",
       "99       56.87181         53.24156     10          7   \n",
       "\n",
       "          district class_nm rooms_cnt area_range_part  flat_cnt  \\\n",
       "0             None     None      None            None       NaN   \n",
       "1             None     None      None            None       NaN   \n",
       "2             None     None      None            None       NaN   \n",
       "3             None     None      None            None       NaN   \n",
       "4        None         1      30  50       1.0   \n",
       "..             ...      ...       ...             ...       ...   \n",
       "95             3      50  70       3.0   \n",
       "96             2      30  50       1.0   \n",
       "97             2      50  70       2.0   \n",
       "98             1      30  50       3.0   \n",
       "99             3      50  70       2.0   \n",
       "\n",
       "    total_area_part  offer_price_sqm_amt offer_currency_price_nm  \\\n",
       "0               NaN                  NaN                    None   \n",
       "1               NaN                  NaN                    None   \n",
       "2               NaN                  NaN                    None   \n",
       "3               NaN                  NaN                    None   \n",
       "4         36.000000         37778.000000                     rur   \n",
       "..              ...                  ...                     ...   \n",
       "95        60.866667         53670.666667                     rur   \n",
       "96        44.500000         50431.000000                     rur   \n",
       "97        55.400000         50111.500000                     rur   \n",
       "98        33.733333         49899.333333                     rur   \n",
       "99        65.500000         26564.500000                     rur   \n",
       "\n",
       "       developer_nm             currency_price_nm deadline_actual  \\\n",
       "0              None                          None            None   \n",
       "1              None                          None            None   \n",
       "2              None                          None            None   \n",
       "3              None                          None            None   \n",
       "4              None                          None            None   \n",
       "..              ...                           ...             ...   \n",
       "95                2020-10-01   \n",
       "96             2020-04-01   \n",
       "97             2020-04-01   \n",
       "98                                    2019-07-01   \n",
       "99                                    2019-07-01   \n",
       "\n",
       "           deadline_change deadline_plan  count_fl_cnt  count_ul_cnt  \\\n",
       "0                     None          None           NaN           NaN   \n",
       "1                     None          None           NaN           NaN   \n",
       "2                     None          None           NaN           NaN   \n",
       "3                     None          None           NaN           NaN   \n",
       "4                     None          None           NaN           NaN   \n",
       "..                     ...           ...           ...           ...   \n",
       "95              2020-10-01    2020-10-01           NaN           NaN   \n",
       "96  2020-04-01, 2019-10-01    2020-04-01           NaN           NaN   \n",
       "97  2020-04-01, 2019-10-01    2020-04-01           NaN           NaN   \n",
       "98              2019-07-01    2019-07-01           NaN           NaN   \n",
       "99              2019-07-01    2019-07-01           NaN           NaN   \n",
       "\n",
       "   bought_price_sqm_info currency_bought_price_nm source_nm   system_dt  \\\n",
       "0                   None                     None      None        None   \n",
       "1                   None                     None      None        None   \n",
       "2                   None                     None      None        None   \n",
       "3                   None                     None      None        None   \n",
       "4                   None                     None        2021-11-25   \n",
       "..                   ...                      ...       ...         ...   \n",
       "95                  None                     None        2021-11-25   \n",
       "96                  None                     None        2021-11-25   \n",
       "97                  None                     None        2021-11-25   \n",
       "98                  None                     None        2021-11-25   \n",
       "99                  None                     None        2021-11-25   \n",
       "\n",
       "        dist  dist_rank  \n",
       "0        NaN          1  \n",
       "1        NaN          1  \n",
       "2        NaN          1  \n",
       "3        NaN          1  \n",
       "4   0.000881          1  \n",
       "..       ...        ...  \n",
       "95  2.284201         38  \n",
       "96  2.284201         38  \n",
       "97  2.284201         38  \n",
       "98  2.284201         38  \n",
       "99  2.284201         38  \n",
       "\n",
       "[100 rows x 33 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.iloc[:100,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADH-USERS Python 3 + PySpark 2",
   "language": "python",
   "name": "python3-pyspark2-adh-users"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
